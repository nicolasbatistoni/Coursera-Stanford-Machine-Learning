
1. 4.

2. θ0=−569.6,θ1=−530.9.

3. 3.

X 4. - If the first few iterations of gradient descent cause f(θ0,θ1) to
     increase rather than decrease, then the most likely cause is that we have set the
     learning rate α to too large a value.
   - No matter how θ0 and θ1 are initialized, so long
     as α is sufficiently small, we can safely expect gradient descent to converge
     to the same solution.

5. - For these values of θ0 and θ1 that satisfy J(θ0,θ1)=0,
     we have that hθ(x(i))=y(i) for every training example (x(i),y(i))

===================================================================================================

1. 4.

2. θ0=0,θ1=0.5.

3. 1.

4. - If the first few iterations of gradient descent cause f(θ0,θ1) to
     increase rather than decrease, then the most likely cause is that we have set the
     learning rate α to too large a value.
   - If θ0 and θ1 are initialized at
     the global minimum, then one iteration will not change their values.

5. - Our training set can be fit perfectly by a straight line,
   i.e., all of our training examples lie perfectly on some straight line.